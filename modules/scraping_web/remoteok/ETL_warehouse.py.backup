from selenium.webdriver.common.by import By
import time
import json
import re
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException, TimeoutException

import logging

logging.basicConfig(level=logging.INFO)

# function to extract salary
def extract_salary(text):
  salary = 'Depend on experience'
  pattern = r'\$\d+k - \$\d+k'
  matches = re.findall(pattern, text)
  if matches:
    salary_range = matches[0]
    return salary_range
  return salary

# function to extract company introduction
# def extract_company_detail(driver, job_link, job_id):
#     job_id_number = job_id.replace("job-", "")
#     driver.get(job_link)
#     selectors = [
#         f"//tbody/tr[starts-with(@class, 'expand expand-{job_id_number}')][contains(@class, 'active')]/td/div[contains(@class, 'expandContents')]/div[contains(@class, 'description')]/div[contains(@class, 'markdown')]/p",
#         f"//tbody/tr[starts-with(@class, 'expand expand-{job_id_number}')][contains(@class, 'active')]/td/div[contains(@class, 'expandContents')]/div[contains(@class, 'description')]/div[contains(@class, 'html')]/div[3]",
#         f"//tbody/tr[starts-with(@class, 'expand expand-{job_id_number}')][contains(@class, 'active')]/td/div[contains(@class, 'expandContents')]/div[contains(@class, 'description')]/div[contains(@class, 'html')]/p"
#     ]
#     for selector in selectors:
#         try:
#             element = WebDriverWait(driver, 15).until(
#                 EC.presence_of_element_located((By.XPATH, selector))
#             )
#             job_company_detail = element.text
#             return job_company_detail
#         except TimeoutException:
#             print(f"TimeoutException occurred for job link: {job_link}")
#             continue
#     print(f"No matching element found for URL: {job_link}")
#     return None

def extract_job_description(driver, job_link, job_id):
    job_id_number = job_id.replace("job-", "")
    driver.get(job_link)
    selectors = [
        f"//tbody/tr[starts-with(@class, 'expand expand-{job_id_number}')][contains(@class, 'active')]/td/div[contains(@class, 'expandContents')]/div[contains(@class, 'description')]"
    ]
    for selector in selectors:
        try:
            element = WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.XPATH, selector))
            )
            job_company_detail = element.text
            return job_company_detail
        except TimeoutException:
            print(f"TimeoutException occurred for job link: {job_link}")
            continue
    print(f"No matching element found for URL: {job_link}")
    return None
  
def run(data_lake, data_warehouse, driver):
  cursor = data_warehouse.cursor()
  collection = data_lake['job_page']
  job_records = collection.find({'website': 'remoteok'})
  
  for page in job_records:
    html_content = page['html_content']
    # Read page source
    driver.execute_script("document.children[0].innerHTML = {}".format(json.dumps(html_content)))
    # Wait for a few seconds to let the page load (adjust as needed)
    time.sleep(15)
    # Get the page source with the dynamically loaded content
    tbody = driver.find_element(By.TAG_NAME, 'tbody')
    list_tr= tbody.find_elements(By.TAG_NAME, 'tr')
    i = 0
    for tr in list_tr:
      logging.info(f"Processing row {i}")
      if tr.get_attribute('data-slug') is not None:
        i+=1
        logging.info(f"Found data-slug in row {i}")
        try:
          id = tr.get_attribute('id')
          logging.info(f"Found id attribute: {id}")
          job_company = ''
          try: 
            job_company = tr.find_element(By.XPATH, f'//*[@id="{id}"]/td[2]/span[3]/h3').text
          except StaleElementReferenceException:
            logging.error("StaleElementReferenceException for job_company. Retrying...")
            time.sleep(1)
            tr = driver.find_element(By.XPATH, f'//*[@id="{id}"]')
            job_company = tr.find_element(By.XPATH, f'//*[@id="{id}"]/td[2]/span[3]/h3').text
          except:
            job_company = tr.find_element(By.XPATH, f'//*[@id="{id}"]/td[2]/span/h3').text

        # try:
          link=tr.get_attribute('data-slug')
          id=tr.get_attribute('id')
          post_tag = tr.find_element(By.TAG_NAME, 'time')
          posted_date = post_tag.get_attribute("datetime")
          job_title =  tr.find_element(By.XPATH, f'//*[@id="{id}"]/td[2]/a/h2').text
          salary= 'Depend on experience'
          location = []  
          job_location=tr.find_elements(By.CLASS_NAME, 'location')
          for loc in job_location:
            if '$' not in loc.text:
              location.append((re.sub(r'[^(a-z|A-Z) ]','', loc.text)).strip())
          salary = extract_salary(tr.text)
          page_link= 'https://remoteok.com/remote-jobs/'
          job_link= page_link + link
          
        except StaleElementReferenceException:
          logging.error(f"StaleElementReferenceException encountered at row {i}")
          time.sleep(1)
          tr = driver.find_element(By.XPATH, f'//*[@id="{id}"]')
          continue
                  
        # Extract of company_detail
        #company_detail = extract_company_detail(driver, job_link, id)
        job_description = extract_job_description(driver, job_link, id)
        
        skill=[]
        h3=tr.find_elements(By.TAG_NAME, 'h3')
        for list in h3:
          if list.text not in job_company:
            elementList = list.text
            skill.append(elementList)
        job_wh_record = {
          'job_link': job_link,
          'job_title':job_title,
          'job_salary':salary,
          'job_location':','.join(location),
          'job_skills':','.join(skill),
          'job_company':job_company,
          'website': page['website'],
          'posted_date':posted_date
        }
        if job_description is not None:
            job_wh_record['job_company_detail'] = job_description
            
        print(job_wh_record)
        data_to_upsert = [(str(job_wh_record['job_salary']), str(job_wh_record['job_company']), str(job_wh_record['job_title']), str(job_wh_record['job_link']),str(job_wh_record['job_location']),str(job_wh_record['job_skills']), str(job_wh_record['website']), job_wh_record['posted_date'])]
        upsert_query = '''
        INSERT INTO job_table (job_salary, job_company, job_title, job_link,job_location,job_skills, website, posted_date)
        VALUES %s
        ON CONFLICT (job_link) DO UPDATE
        SET job_salary = EXCLUDED.job_salary,
            job_company = EXCLUDED.job_company,
            job_title = EXCLUDED.job_title,
            job_location = EXCLUDED.job_location,
            job_skills= EXCLUDED.job_skills,
            website = EXCLUDED.website,
            posted_date = EXCLUDED.posted_date;
        '''
        cursor.execute(upsert_query, data_to_upsert)
        data_warehouse.commit()
        # job_collection.update_one({'job_link': job_wh_record['job_link']}, {'$set': {'ETL': True}}, upsert=True)
  
